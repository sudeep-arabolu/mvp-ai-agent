# AIâ€¯Analyticsâ€¯AgentÂ (MVP)

> Conversational assistant that lets programâ€‘management teams ask plainâ€‘English questions and receive **SQLâ€‘grade numbers _plus_ narrative context** in real time.

---

## ğŸŒ  Project Overview
Traditional BI dashboards show numbers but hide the â€œwhy.â€  
This MVP fuses **warehouse accuracy** with **LLM reasoning**:

* Exact KPIs (budget, velocity, MTTRâ€¦) come straight from Aurora queries.  
* Explanations are enriched with risk descriptions, milestone text, OKRsâ€”retrieved semantically from pgvector embeddings.  
* A single endpoint streams answers tokenâ€‘byâ€‘token to the UI, producing a ChatGPTâ€‘like experience without sacrificing data fidelity.

---

## âœ¨  Key Features

| Feature | Benefit |
|---------|---------|
| **Hybrid Retrieval (SQLÂ +Â Vector)** | Numbers never hallucinate; narrative answers stay relevant. |
| **Streaming Serverâ€‘Sent Events** | Users see answers materialise in <â€¯1â€¯s, boosting perceived speed. |
| **No Ops for MVP** | Runs locally or in one ECSâ€¯Fargate task; hourly Lambda keeps embeddings fresh. |
| **Cloudâ€‘native Security** | All data stays inside AWS; IAM + SecretsÂ Manager, no external keys. |

---

## ğŸ¯  System Design Goals

| Goal | Design Choice |
|------|---------------|
| **Accuracy first** | Route numeric questions to SQL, not the LLM. |
| **Explainability** | Attach the text snippet/source behind every answer. |
| **Costâ€‘efficiency** | pgvector inside Aurora until >â€¯3â€¯M vectors, then OpenSearch. |
| **Incremental growth** | LangChain tools let us bolt on Jira, GitHub, SAP via config, not code rewrites. |
| **Fast POC â†’ easy prod** | Same FastAPI code runs on laptop, Lambda, or Fargate. |

---
## ğŸ—ï¸ System Design Principles

- **Data Ingestion Pipeline**  
  Raw data lands in S3, is processed by an hourly Lambda into a staging schema, then loaded into Auroraâ€™s star schema for analytics.

- **Hybrid Agent Architecture**  
  Combines a reactive layer (instant SQL lookups) with a deliberative layer (semantic search + LLM synthesis) for speed and depth.

- **Tool Use Pattern**  
  Agent dynamically chooses SQL or vector search per question, enabling plug-and-play integrations via config.

- **Modularity & Scalability**  
  Retrieval, embedding, orchestration, and UI are independent services-easy to replace, scale, and maintain.

- **Observability & Robustness**  
  Centralized logging, version-pinned LLMs, and secure secrets management ensure reliability in production.

- **User-Centric Design**  
  Explainable responses build trust and engagement

--- 
## ğŸ§© Design Patterns Used

- **Ingestion Pipeline Pattern**: Raw data in S3 â†’ staging schema via Lambda â†’ analytics schema in Aurora.
- **Hybrid Agent**: Reactive + deliberative layers for precise KPIs and rich context.  
- **Tool Use**: Dynamic routing to SQL or vector tools; config-driven integration points.  
- **Planning**: Multi-step decomposition for complex queries (aggregate metrics â†’ contextualize).  
- **Microservices**: Independent services for each concern-retrieval, embedding, orchestration, UI.

---
## ğŸ–¼ï¸  Architecture

```mermaid
flowchart TD
    subgraph Browser
        FE["React UI<br/>(useChat hook)"]
        FE -- "POST /ask<br/>(SSE)" --> APIGW["APIÂ Gateway*"]
    end

    subgraph Backend
        APIGW --> AGENT["FastAPI<br/>Hybrid Agent"]
        AGENT -- "SQL" --> DB[(Aurora<br/>Starâ€‘Schema)]
        AGENT -- "kâ€‘NN" --> VEC[(pgvector<br/>Embeddings)]
        AGENT --> LLM["Bedrock<br/>ClaudeÂ 3Â Haiku"]
    end

    VEC --- EMBED[[Bedrock<br/>CohereÂ v3Â Large]]

    subgraph Jobs
        EMBJOB["smart_embed.py<br/>Hourly Lambda"]
        DB -.-> EMBJOB
        EMBJOB -.-> VEC
    end


